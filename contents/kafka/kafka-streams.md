# Kafka Streams 에 대한 고찰

# Kafka Streams❓

> 토픽에 적재된 데이터를 기반으로 **상태기반** 또는 **비상태기반**으로 실시간 변환하여 다른 토픽에 적재하는 라이브러리

💡 상태 기반❓
- 상태 저장소(State Store):
  - RocksDB 같은 임베디드 데이터베이스를 사용하여 처리 과정에서 발생하는 상태 정보를 저장
- KTable:
  - 키-값 형태의 데이터를 테이블처럼 관리하는 추상화
  - 입력 스트림의 각 키에 대한 최신 상태를 저장하고 관리하며, 상태 저장소에 저장
- 변경 로그 토픽(Changelog Topic):
  - 상태 저장소에 저장된 상태 변경 사항을 기록(내부적으로 토픽 생성)
  - 애플리케이션이 재시작되거나 장애가 발생했을 때 상태를 복원하는 데 사용
- 윈도우 기반 처리(Windowing):
  - 특정 시간 범위 또는 이벤트 범위 내에서 상태를 관리하여, 특정 기간 동안의 데이터 집계, 추이 분석 등을 수행 가능

💡 상태 기반 처리에 활용될 수 있는 사례❓
- **실시간 데이터 집계 및 분석**
  - 특정 기간 동안의 데이터 집계: 시간별, 분별, 일별 사용자 활동, 매출 현황 등
  - 실시간 추이 분석: 주식 가격 변동, 웹사이트 트래픽 변화 등을 실시간으로 감지하고 분석
  - 이상 감지: 사용자 행동 패턴, 시스템 로그 등을 분석하여 비정상적인 활동을 실시간으로 감지
- **실시간 데이터 변환 및 처리**
  - 데이터 스트림 조인: 여러 데이터 스트림을 결합하여 새로운 스트림을 생성
  - 데이터 필터링 및 변환: 특정 조건에 맞는 데이터만 필터링하거나, 데이터를 다른 형식으로 변환
  - 세션 기반 처리: 사용자 세션, 네트워크 세션 등 특정 세션 동안의 데이터를 처리
- **실시간 추천 시스템**
  - 사용자 행동 패턴 분석: 사용자의 과거 구매 내역, 검색 기록 등을 분석하여 실시간으로 상품을 추천
  - 실시간 광고 타겟팅: 사용자 위치, 관심사 등을 분석하여 실시간으로 광고를 노출
- **실시간 모니터링 및 알림**
  - 시스템 모니터링: CPU 사용량, 메모리 사용량, 네트워크 트래픽 등을 실시간으로 모니터링하고 이상 발생 시 알림을 전송
  - 금융 거래 모니터링: 실시간으로 금융 거래를 모니터링하고 이상 거래 감지 시 알림을 전송

## 토폴로지

> 데이터 스트림을 처리하는 과정, 즉 데이터의 흐름과 변환 과정을 정의하는 구조






processor: 카프카 스트림즈에서 토폴로지를 이루는 노드
stream: 노드와 노드를 이은 선
스트림은 토픽의 데이터를 뜻하는데 프로듀서와 컨슈머에서 활용했던 레코드와 동일

프로세서에서 소스 프로세서, 스트림 프로세서, 싱크 프로세서 세 가지가 존재
소스 프로세스

데이터를 처리하기 위해 최초로 선언해야 하는 노드
하나 이상의 토픽에서 데이터를 가져오는 역할
스트림 프로세스

다른 프로세서가 반환한 데이터를 처리하는 역할
변환, 분기처리와 같은 로직이 데이터 처리의 일종
싱크 프로세서

데이터를 특정 카프카 토픽으로 저장하는 역할
스트림즈로 처리된 데이터의 최종 종착지\

이미지






`Streams DSL`의 `KStream`을 활용하여 구현


레코드의 흐름을 표현한 것으로 메시지 키와 메시지 값으로 구성
KStream으로 데이터를 조회하면 토픽에 존재하는(또는 KStream에 존재하는) 모든 레코드가 출력

## vs Batch


## vs Producer/Consumer